<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Shane Dowling - homelab</title><link href="https://shanedowling.com/" rel="alternate"></link><link href="https://shanedowling.com/feeds/homelab.atom.xml" rel="self"></link><id>https://shanedowling.com/</id><updated>2025-06-18T21:11:54+00:00</updated><subtitle>Platform Engineering Manager</subtitle><entry><title>Homelab - Day 5 - The NAS</title><link href="https://shanedowling.com/homelab---day-5---the-nas.html" rel="alternate"></link><published>2025-06-18T21:11:54+00:00</published><updated>2025-06-18T21:11:54+00:00</updated><author><name>Shane Dowling</name></author><id>tag:shanedowling.com,2025-06-18:/homelab---day-5---the-nas.html</id><summary type="html">&lt;p&gt;&lt;img src="images/hp-proliant.jpg" height="400px"&gt;&lt;/p&gt;
&lt;p&gt;So, it's been a while and I have made some progress worth sharing!&lt;/p&gt;
&lt;p&gt;After keeping an eye on the auction sites for a while I ended up pulling the plug and buying 2 12TB Seagate Ironwolf drives for my NAS and now wanted to get started with setting this up …&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;img src="images/hp-proliant.jpg" height="400px"&gt;&lt;/p&gt;
&lt;p&gt;So, it's been a while and I have made some progress worth sharing!&lt;/p&gt;
&lt;p&gt;After keeping an eye on the auction sites for a while I ended up pulling the plug and buying 2 12TB Seagate Ironwolf drives for my NAS and now wanted to get started with setting this up.&lt;/p&gt;
&lt;h2&gt;The hardware&lt;/h2&gt;
&lt;p&gt;Off a recommendation from Joe at &lt;a href="https://latenightlinux.com/"&gt;Late Night Linux&lt;/a&gt; I picked up a HP 290 G2 I5-8500. It comes with 8GB of RAM and a 256ssd. The form factor is reasonably nice and it fit the two drives perfectly and it's been chugging along well.&lt;/p&gt;
&lt;h2&gt;The software&lt;/h2&gt;
&lt;p&gt;As this was very much a learning experience for me I wanted to do this from scratch. So I opted for something familiar OS wise so I could focus more on getting the NAS itself setup.&lt;/p&gt;
&lt;p&gt;It's running Ubuntu 24.04 LTS and I have the NAS drives setup using as ZFS mirrors. Based also on Joe's suggestion I set it up using dataset, this allows you to logically separate your ZFS drive and apply different rules to them, eg compression or backups depending on the data you're storing.&lt;/p&gt;
&lt;p&gt;What it might look like:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;sudo&lt;span class="w"&gt; &lt;/span&gt;zfs&lt;span class="w"&gt; &lt;/span&gt;create&lt;span class="w"&gt; &lt;/span&gt;nas/media
sudo&lt;span class="w"&gt; &lt;/span&gt;zfs&lt;span class="w"&gt; &lt;/span&gt;create&lt;span class="w"&gt; &lt;/span&gt;nas/important
sudo&lt;span class="w"&gt; &lt;/span&gt;zfs&lt;span class="w"&gt; &lt;/span&gt;create&lt;span class="w"&gt; &lt;/span&gt;nas/k8s
sudo&lt;span class="w"&gt; &lt;/span&gt;zfs&lt;span class="w"&gt; &lt;/span&gt;create&lt;span class="w"&gt; &lt;/span&gt;nas/tmp
sudo&lt;span class="w"&gt; &lt;/span&gt;zfs&lt;span class="w"&gt; &lt;/span&gt;create&lt;span class="w"&gt; &lt;/span&gt;nas/backups
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Then setting different rules like compression would be:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;sudo&lt;span class="w"&gt; &lt;/span&gt;zfs&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nb"&gt;set&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;compression&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;lz4&lt;span class="w"&gt; &lt;/span&gt;nas/media
sudo&lt;span class="w"&gt; &lt;/span&gt;zfs&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nb"&gt;set&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;compression&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;lz4&lt;span class="w"&gt; &lt;/span&gt;nas/important
sudo&lt;span class="w"&gt; &lt;/span&gt;zfs&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nb"&gt;set&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;compression&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;lz4&lt;span class="w"&gt; &lt;/span&gt;nas/k8s
sudo&lt;span class="w"&gt; &lt;/span&gt;zfs&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nb"&gt;set&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;compression&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;lz4&lt;span class="w"&gt; &lt;/span&gt;nas/backups
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;To do snapshot you install zfs-auto-snapshot and you can then set snapshot policies like so:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;sudo&lt;span class="w"&gt; &lt;/span&gt;zfs&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nb"&gt;set&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;com.sun:auto-snapshot&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nb"&gt;true&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;nas/important
sudo&lt;span class="w"&gt; &lt;/span&gt;zfs&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nb"&gt;set&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;com.sun:auto-snapshot&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nb"&gt;true&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;nas/k8s/nextcloud
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;To actually serve the files I'm using nfs, which is actually super easy to install, run and configure.&lt;/p&gt;
&lt;h2&gt;Next steps&lt;/h2&gt;
&lt;p&gt;Okay so I've got my (hopefully) reliable data storage finally sorted, now it's time to get back to the K8s cluster and see if I can build something helpful.&lt;/p&gt;</content><category term="homelab"></category></entry><entry><title>Homelab - Day 4 - Self-Hosting Gitea</title><link href="https://shanedowling.com/homelab---day-4---self-hosting-gitea.html" rel="alternate"></link><published>2025-03-07T19:31:59+00:00</published><updated>2025-03-07T19:31:59+00:00</updated><author><name>Shane Dowling</name></author><id>tag:shanedowling.com,2025-03-07:/homelab---day-4---self-hosting-gitea.html</id><summary type="html">&lt;h1&gt;Aims&lt;/h1&gt;
&lt;p&gt;Today’s goal was to deploy Gitea on my K3s cluster, ensuring that:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;It used persistent storage via my NAS.&lt;/li&gt;
&lt;li&gt;It was accessible externally through Traefik.&lt;/li&gt;
&lt;li&gt;It could eventually replace GitHub for my FluxCD setup.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;What I Did&lt;/h1&gt;
&lt;h2&gt;Storage and Permissions Woes&lt;/h2&gt;
&lt;p&gt;Running Gitea as a stateless container worked …&lt;/p&gt;</summary><content type="html">&lt;h1&gt;Aims&lt;/h1&gt;
&lt;p&gt;Today’s goal was to deploy Gitea on my K3s cluster, ensuring that:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;It used persistent storage via my NAS.&lt;/li&gt;
&lt;li&gt;It was accessible externally through Traefik.&lt;/li&gt;
&lt;li&gt;It could eventually replace GitHub for my FluxCD setup.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;What I Did&lt;/h1&gt;
&lt;h2&gt;Storage and Permissions Woes&lt;/h2&gt;
&lt;p&gt;Running Gitea as a stateless container worked fine initially, but as soon as I introduced persistence, things got tricky. I ran into permission issues almost immediately, thanks to the interplay between Kubernetes' user permissions and my NAS file system.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;I set up a PersistentVolume (PV) backed by NFS, but the default Gitea user didn’t have the correct write permissions.&lt;/li&gt;
&lt;li&gt;After some trial and error (and some questionable security decisions), I got it working by setting specific user IDs and group IDs on the NAS side to align with the Gitea container.&lt;/li&gt;
&lt;li&gt;Long-term, I’ll need a more secure approach, likely involving init containers to set permissions properly.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Exposing Gitea with Traefik&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Since K3s includes Traefik by default, I wanted to use it to expose Gitea externally.&lt;/li&gt;
&lt;li&gt;Unlike Unifi, which required &lt;code&gt;IngressRouteTCP&lt;/code&gt;, Gitea worked fine with a standard Ingress configuration.&lt;/li&gt;
&lt;li&gt;I configured DNS to point &lt;code&gt;git.example.com&lt;/code&gt; to my Traefik instance, and everything worked as expected.&lt;/li&gt;
&lt;li&gt;Here’s the working setup:&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nt"&gt;apiVersion&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;networking.k8s.io/v1&lt;/span&gt;
&lt;span class="nt"&gt;kind&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;Ingress&lt;/span&gt;
&lt;span class="nt"&gt;metadata&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
&lt;span class="nt"&gt;name&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;gitea-ingress&lt;/span&gt;
&lt;span class="nt"&gt;namespace&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;gitea&lt;/span&gt;
&lt;span class="nt"&gt;spec&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
&lt;span class="nt"&gt;rules&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="p p-Indicator"&gt;-&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nt"&gt;host&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;git.example.com&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nt"&gt;http&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
&lt;span class="w"&gt;      &lt;/span&gt;&lt;span class="nt"&gt;paths&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="p p-Indicator"&gt;-&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nt"&gt;path&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;/&lt;/span&gt;
&lt;span class="w"&gt;          &lt;/span&gt;&lt;span class="nt"&gt;pathType&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;Prefix&lt;/span&gt;
&lt;span class="w"&gt;          &lt;/span&gt;&lt;span class="nt"&gt;backend&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
&lt;span class="w"&gt;            &lt;/span&gt;&lt;span class="nt"&gt;service&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
&lt;span class="w"&gt;              &lt;/span&gt;&lt;span class="nt"&gt;name&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;gitea-web&lt;/span&gt;
&lt;span class="w"&gt;              &lt;/span&gt;&lt;span class="nt"&gt;port&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
&lt;span class="w"&gt;                &lt;/span&gt;&lt;span class="nt"&gt;number&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;3000&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h1&gt;Next Steps: FluxCD Integration&lt;/h1&gt;
&lt;p&gt;Now that Gitea is running, the next challenge is configuring my FluxCD setup to pull from it rather than GitHub.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;I’ll need to generate an SSH keypair for Flux and store the public key in Gitea.&lt;/li&gt;
&lt;li&gt;Update my &lt;code&gt;GitRepository&lt;/code&gt; resources in Flux to use the self-hosted Gitea instance.&lt;/li&gt;
&lt;li&gt;Verify that Flux can pull manifests and reconcile changes correctly.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;Lessons Learned&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Storage &amp;amp; Permissions:&lt;/strong&gt; Dealing with NAS-mounted volumes on Kubernetes often introduces user/group ID mismatches. Ensuring correct ownership at the NAS level is crucial.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Security Considerations:&lt;/strong&gt; In my initial setup, I relaxed permissions too much just to get things running. A proper solution will involve configuring the correct UID/GID mappings in a way that doesn’t compromise security.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Ingress Simplicity:&lt;/strong&gt; Unlike Unifi, which required TCP passthrough for TLS termination, Gitea was happy with a simple &lt;code&gt;Ingress&lt;/code&gt; resource.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;Final Thoughts&lt;/h1&gt;
&lt;p&gt;With Gitea running smoothly, I’m one step closer to fully self-hosting my development workflow. Getting FluxCD working with it will be the next major milestone. Beyond that, I’ll need to consider backups, access controls, and high availability—but that’s a problem for another day!&lt;/p&gt;</content><category term="homelab"></category></entry><entry><title>Homelab - Day 3 - The first software deployment</title><link href="https://shanedowling.com/homelab---day-3---the-first-software-deployment.html" rel="alternate"></link><published>2025-02-28T10:45:34+00:00</published><updated>2025-02-28T10:45:34+00:00</updated><author><name>Shane Dowling</name></author><id>tag:shanedowling.com,2025-02-28:/homelab---day-3---the-first-software-deployment.html</id><summary type="html">&lt;h1&gt;Aims&lt;/h1&gt;
&lt;p&gt;The goal for today was to get my &lt;strong&gt;first piece of software running on the cluster&lt;/strong&gt;, ensuring that:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;It leveraged the &lt;strong&gt;NAS for storage&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;It was accessible externally via &lt;strong&gt;Traefik&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;The process was repeatable for future deployments.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I chose the &lt;strong&gt;Unifi Management Console&lt;/strong&gt; as my first self-hosted application. Once …&lt;/p&gt;</summary><content type="html">&lt;h1&gt;Aims&lt;/h1&gt;
&lt;p&gt;The goal for today was to get my &lt;strong&gt;first piece of software running on the cluster&lt;/strong&gt;, ensuring that:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;It leveraged the &lt;strong&gt;NAS for storage&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;It was accessible externally via &lt;strong&gt;Traefik&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;The process was repeatable for future deployments.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I chose the &lt;strong&gt;Unifi Management Console&lt;/strong&gt; as my first self-hosted application. Once migrated, I could &lt;strong&gt;wipe my Raspberry Pi 4&lt;/strong&gt;, which was previously running the console, and add it as a &lt;strong&gt;worker node to the cluster&lt;/strong&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;h1&gt;&lt;strong&gt;What I Did&lt;/strong&gt;&lt;/h1&gt;
&lt;h2&gt;&lt;strong&gt;NFS Storage Setup&lt;/strong&gt;&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;I needed &lt;strong&gt;NFS storage&lt;/strong&gt; to persist Unifi’s data.&lt;/li&gt;
&lt;li&gt;After trying multiple providers, I settled on the &lt;strong&gt;nfs-subdir-external-provisioner&lt;/strong&gt; Helm chart.&lt;/li&gt;
&lt;li&gt;I pointed it at my NAS server to handle persistent storage.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;strong&gt;Persistent Volume &amp;amp; Claims&lt;/strong&gt;&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Configuring &lt;strong&gt;Persistent Volumes (PVs) and Persistent Volume Claims (PVCs)&lt;/strong&gt; took some effort.&lt;/li&gt;
&lt;li&gt;Eventually, I got them working with my NAS-hosted NFS.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;strong&gt;Ingress &amp;amp; Networking Challenges&lt;/strong&gt;&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Since K3s includes &lt;strong&gt;Traefik by default&lt;/strong&gt;, I wanted to leverage its &lt;strong&gt;built-in ingress controller&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;Initially, I attempted to use &lt;strong&gt;IngressRoutes&lt;/strong&gt;, but Unifi’s &lt;strong&gt;self-signed TLS certificate&lt;/strong&gt; caused issues.&lt;/li&gt;
&lt;li&gt;After much debugging, I realised I needed &lt;strong&gt;IngressRouteTCP&lt;/strong&gt; with &lt;strong&gt;TLS passthrough&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;The final working configuration looked like this:&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nt"&gt;apiVersion&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;traefik.containo.us/v1alpha1&lt;/span&gt;
&lt;span class="nt"&gt;kind&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;IngressRouteTCP&lt;/span&gt;
&lt;span class="nt"&gt;metadata&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;name&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;unifi-ingress-tcp&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;namespace&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;unifi&lt;/span&gt;
&lt;span class="nt"&gt;spec&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;entryPoints&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="p p-Indicator"&gt;-&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;websecure&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;routes&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="p p-Indicator"&gt;-&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nt"&gt;match&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;HostSNI(`example.com`)&lt;/span&gt;&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="c1"&gt;# SNI-based routing required&lt;/span&gt;
&lt;span class="w"&gt;      &lt;/span&gt;&lt;span class="nt"&gt;services&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="p p-Indicator"&gt;-&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nt"&gt;name&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;unifi-gui&lt;/span&gt;
&lt;span class="w"&gt;          &lt;/span&gt;&lt;span class="nt"&gt;port&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;8443&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;tls&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nt"&gt;passthrough&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;true&lt;/span&gt;&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="c1"&gt;# Required to let Unifi handle TLS&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;hr&gt;
&lt;h1&gt;&lt;strong&gt;Lessons Learned&lt;/strong&gt;&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;NFS Setup Took Iterations&lt;/strong&gt; – The &lt;strong&gt;nfs-subdir-external-provisioner&lt;/strong&gt; was the best option for easy NAS integration.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Ingress Troubles&lt;/strong&gt; – K3s’ built-in &lt;strong&gt;Traefik&lt;/strong&gt; works well, but TLS handling is tricky with &lt;strong&gt;self-signed certs&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;IngressRoute vs IngressRouteTCP&lt;/strong&gt; – If an app &lt;strong&gt;manages its own TLS&lt;/strong&gt;, use &lt;strong&gt;IngressRouteTCP with passthrough&lt;/strong&gt; instead of a standard &lt;strong&gt;IngressRoute&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h1&gt;&lt;strong&gt;Final Thoughts&lt;/strong&gt;&lt;/h1&gt;
&lt;p&gt;Now that I’ve got my first app running, I expect future deployments to be &lt;strong&gt;faster and more repeatable&lt;/strong&gt;. Next, I need to figure out &lt;strong&gt;subdomains and path-based routing&lt;/strong&gt; for managing multiple applications—but that’s a problem for another day!&lt;/p&gt;</content><category term="homelab"></category></entry><entry><title>Homelab - Day 2 - Setting Up a New K3s Control Plane and Node</title><link href="https://shanedowling.com/homelab---day-2---setting-up-k3s-control-plane.html" rel="alternate"></link><published>2025-02-21T23:57:09+00:00</published><updated>2025-02-21T23:57:09+00:00</updated><author><name>Shane Dowling</name></author><id>tag:shanedowling.com,2025-02-21:/homelab---day-2---setting-up-k3s-control-plane.html</id><summary type="html">&lt;p&gt;After going back and forth multiple times and making plenty of mistakes, I finally managed to set up a new K3s control plane and node. It turned out to be relatively straightforward once I got the steps right, so I’m documenting this for future reference!&lt;/p&gt;
&lt;h2&gt;Aims&lt;/h2&gt;
&lt;p&gt;The goal was …&lt;/p&gt;</summary><content type="html">&lt;p&gt;After going back and forth multiple times and making plenty of mistakes, I finally managed to set up a new K3s control plane and node. It turned out to be relatively straightforward once I got the steps right, so I’m documenting this for future reference!&lt;/p&gt;
&lt;h2&gt;Aims&lt;/h2&gt;
&lt;p&gt;The goal was to set up a fresh K3s cluster, ensuring that:
- The control plane and node are configured correctly.
- Flux is properly bootstrapped to manage cluster state.
- Common pitfalls are documented to save time in the future.&lt;/p&gt;
&lt;h2&gt;What I Did&lt;/h2&gt;
&lt;h3&gt;Control Plane Setup&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;Install K3s:
   &lt;code&gt;sh
   curl -sfL https://get.k3s.io | sh&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Add an SSH key for Flux to connect to GitHub.&lt;/li&gt;
&lt;li&gt;Bootstrap Flux with Git:
   &lt;code&gt;sh
   flux bootstrap git \
     --url=ssh://git@github.com/&amp;lt;your-repo&amp;gt; \
     --branch=main \
     --private-key-file=/home/&amp;lt;user&amp;gt;/.ssh/&amp;lt;flux_key&amp;gt; \
     --path=clusters/k3s&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Retrieve the control plane token:
   &lt;code&gt;sh
   cat /var/lib/rancher/k3s/server/token&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Allow traffic on the K3s API port:
   &lt;code&gt;sh
   iptables -A INPUT -p tcp --dport 6443 -j ACCEPT&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;Node Setup&lt;/h3&gt;
&lt;p&gt;To join a new node to the cluster:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nv"&gt;K3S_TOKEN&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&amp;lt;your-k3s-token&amp;gt;&amp;quot;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="se"&gt;\&lt;/span&gt;
&lt;span class="nv"&gt;K3S_URL&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&amp;lt;your-k3s-url&amp;gt;&amp;quot;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="se"&gt;\&lt;/span&gt;
sh&lt;span class="w"&gt; &lt;/span&gt;-c&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="k"&gt;$(&lt;/span&gt;curl&lt;span class="w"&gt; &lt;/span&gt;-sfL&lt;span class="w"&gt; &lt;/span&gt;https://get.k3s.io&lt;span class="k"&gt;)&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h2&gt;Lessons Learned&lt;/h2&gt;
&lt;h3&gt;CA Certificates Issues&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;I kept running into problems with CA certificates not being authorized. This was likely due to wiping the server and recreating it multiple times or some Flux configuration persisting in the repository.&lt;/li&gt;
&lt;li&gt;Fix:
 &lt;code&gt;sh
 sudo cp &amp;lt;path-to-ca-cert&amp;gt; &amp;lt;path-to-cert-destination&amp;gt;
 sudo update-ca-certificates&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Image Pull Failures&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;At one point, I was pulling my hair out trying to figure out why I couldn't pull container images. After a lot of debugging, it turned out that &lt;code&gt;docker.io&lt;/code&gt; was simply down.&lt;/li&gt;
&lt;li&gt;Lesson: Always check by running:
 &lt;code&gt;sh
 ping docker.io&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Sometimes (though rarely), it's not your fault!&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Final Thoughts&lt;/h2&gt;
&lt;p&gt;Despite the initial struggles, setting up K3s is quite straightforward. The biggest challenges I faced were external issues (e.g., Docker Hub being down) and ensuring CA certificates were recognized properly. Hopefully, this documentation will make things smoother next time!&lt;/p&gt;
&lt;p&gt;Have you run into similar K3s setup challenges? Let me know how you solved them!&lt;/p&gt;</content><category term="homelab"></category></entry><entry><title>Homelab - Day 1 - Really Ghetto Data Storage</title><link href="https://shanedowling.com/homelab---day-1---data-storage.html" rel="alternate"></link><published>2025-02-11T13:57:09+00:00</published><updated>2025-02-11T13:57:09+00:00</updated><author><name>Shane Dowling</name></author><id>tag:shanedowling.com,2025-02-11:/homelab---day-1---data-storage.html</id><summary type="html">&lt;h2&gt;Setting Up Storage for the Kubernetes Cluster&lt;/h2&gt;
&lt;p&gt;The first step in setting up my homelab Kubernetes cluster is ensuring some reliable data storage. Right now, I have two main categories of data in mind:&lt;/p&gt;
&lt;h3&gt;1. Operationally Critical Data&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Needs RAID with redundancy&lt;/li&gt;
&lt;li&gt;Requires both on-site and off-site backups&lt;/li&gt;
&lt;li&gt;Approximately &lt;strong&gt;1TB …&lt;/strong&gt;&lt;/li&gt;&lt;/ul&gt;</summary><content type="html">&lt;h2&gt;Setting Up Storage for the Kubernetes Cluster&lt;/h2&gt;
&lt;p&gt;The first step in setting up my homelab Kubernetes cluster is ensuring some reliable data storage. Right now, I have two main categories of data in mind:&lt;/p&gt;
&lt;h3&gt;1. Operationally Critical Data&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Needs RAID with redundancy&lt;/li&gt;
&lt;li&gt;Requires both on-site and off-site backups&lt;/li&gt;
&lt;li&gt;Approximately &lt;strong&gt;1TB&lt;/strong&gt; of data&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;2. Media Data&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Non-critical, so no backups required&lt;/li&gt;
&lt;li&gt;Several terabytes of storage needed&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Since the Kubernetes cluster depends on Operationally Critical Data, I need to establish a RAID-backed storage solution with proper backups before moving forward.&lt;/p&gt;
&lt;h2&gt;What I’ve Done So Far&lt;/h2&gt;
&lt;p&gt;In the spirit of keeping things cheap and hacky, I'm repurposing an old ThinkPad X230 as my server, paired with a few USB drives for RAID storage. Here’s what I’ve done:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Installed OpenMediaVault on the ThinkPad&lt;/li&gt;
&lt;li&gt;Wiped and configured two 500GB hard drives and one 1TB hard drive&lt;/li&gt;
&lt;li&gt;Set up RAID 5, providing 1TB of usable storage&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Key Learnings&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;USB RAID is a really dodgy solution for data storage&lt;/strong&gt; – The OpenMediaVault UI didn’t even allow it, though setting it up via CLI was easy. This reinforces my concern that &lt;strong&gt;USB RAID isn’t a viable long-term solution&lt;/strong&gt;. So I guess I've learned I need to buy a NAS asap.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;RAID setup was easier than expected&lt;/strong&gt; – I’ve avoided running RAID for years, but getting it working was surprisingly straightforward.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Networking will need an upgrade&lt;/strong&gt; – My BT Hub router and a small 5-in-1 switch aren’t going to cut it. A proper network switch may need to be another purchase.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This is just the beginning, but storage is a critical first step. On to the next challenge!&lt;/p&gt;</content><category term="homelab"></category></entry><entry><title>Homelab Setup: The Journey to Kubernetes</title><link href="https://shanedowling.com/homelab-setup-the-journey-to-kubernetes.html" rel="alternate"></link><published>2025-02-10T20:50:46+00:00</published><updated>2025-02-10T20:50:46+00:00</updated><author><name>Shane Dowling</name></author><id>tag:shanedowling.com,2025-02-10:/homelab-setup-the-journey-to-kubernetes.html</id><summary type="html">&lt;h2&gt;Why I'm Migrating My Self-Hosted Services&lt;/h2&gt;
&lt;p&gt;I've decided to migrate all my existing self-hosted services—and add a few new ones—to a Kubernetes (K8s) cluster that I'm going to build. There are several motivations behind this decision:&lt;/p&gt;
&lt;h3&gt;Learning Kubernetes Beyond Ephemeral Clusters&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;My role as an Engineering Manager at …&lt;/li&gt;&lt;/ul&gt;</summary><content type="html">&lt;h2&gt;Why I'm Migrating My Self-Hosted Services&lt;/h2&gt;
&lt;p&gt;I've decided to migrate all my existing self-hosted services—and add a few new ones—to a Kubernetes (K8s) cluster that I'm going to build. There are several motivations behind this decision:&lt;/p&gt;
&lt;h3&gt;Learning Kubernetes Beyond Ephemeral Clusters&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;My role as an Engineering Manager at Syntasso involves a lot of K8s development using Kind, but these environments are entirely ephemeral. I want to experience the long-term challenges of operating a cluster to build greater empathy for our customers.&lt;/li&gt;
&lt;li&gt;I want to deepen my understanding of Kubernetes, particularly in areas like:&lt;ul&gt;
&lt;li&gt;Hardening a K8s cluster&lt;/li&gt;
&lt;li&gt;Managing multiple clusters effectively&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Increased Focus on Self-Hosting&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;The British government's baffling approach to tech privacy has pushed me toward self-hosting more of my personal services.&lt;/li&gt;
&lt;li&gt;My current setup is a mix of manually configured systems and Docker Compose-based services running on different machines. It’s time to bring structure to the chaos.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;The Plan&lt;/h2&gt;
&lt;h3&gt;Time Constraints and Approach&lt;/h3&gt;
&lt;p&gt;I'm a dad with two small kids, so my time for this project is limited. My approach will be:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Chipping away at it&lt;/strong&gt; for a few minutes each night.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Updating my progress&lt;/strong&gt; as I go along.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Keeping it cheap&lt;/strong&gt;—I'll work with my existing hardware for now. If this experiment proves successful, I may invest in better hardware later.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Minimising downtime&lt;/strong&gt;—I already have services running that I need to migrate carefully without disruptions.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Migration Phases&lt;/h3&gt;
&lt;p&gt;The migration will be broken down into multiple phases to ensure a smooth transition. My current focus is on:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Setting up RAID storage&lt;/strong&gt; for critical data (Nextcloud, Git-Tea, Pixelfed, databases) on an old laptop.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Keeping USB media drives on a thin client&lt;/strong&gt; until I migrate services to Kubernetes.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Moving USB media drives to a MergerFS pool&lt;/strong&gt; for media storage before the final migration.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Building a Kubernetes cluster (Talos OS)&lt;/strong&gt; once storage is stable.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Migrating services from Docker Compose&lt;/strong&gt; to Kubernetes with proper storage integration.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;Rough Architecture&lt;/h2&gt;
&lt;p&gt;At the moment, everything will sit under the same LAN at home. However, as I progress, I may invest in a switch and introduce proper network segmentation.&lt;/p&gt;
&lt;p&gt;This is just the beginning. I'll document my learnings, mistakes, and adjustments along the way. Stay tuned!&lt;/p&gt;</content><category term="homelab"></category></entry></feed>